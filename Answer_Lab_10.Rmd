---
title: "STATS 769 Lab 10"
author: "Wennan Shi"
date: "24/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Data

We will continue to use the two Zip code datasets that we have used in the past few labs.

```{r, message=FALSE}
library(keras)
```

## Introduction

1. In your own words, describe briefly the data and the data mining problems that are studied in this lab.

The dataset ```ziptrain.csv``` has 7291 observations and ```ziptest.csv``` has 2007 observations. They both have 257 numerical variables. ```digit``` is a response variable in the range 0-9.

After extract the corresponding training and test subsets, the dataset ```train``` has 1389 observations and ```test``` has 364 observations. They both have 257 numerical variables. ```digit``` is a response variable in the range 2 and 3, originally it is int type, we need to convert it to factor type.

## An Image

2. Produce a 10x10 image that looks similar to the following, where each handwritten digit is randomly chosen from the subset with its corresponding numeral value in the training set.

```{r}
train = read.csv("ziptrain.csv")
test = read.csv("ziptest.csv")
```

```{r}
par(mar=rep(0,4),mfrow=c(10,10))
train.x = as.matrix((train[,-1]+1)/2)
train.y = as.matrix((train[,1]))
dim(train.x) <- c(7291,16,16,1)
train.x <- aperm(train.x,c(1,3,2,4))
classes = rep(0:9,1) # class label
for(j in 1:10){
	k=sample(which(train.y == classes[j]),10)
	for (i in k)
	plot(as.raster(train.x[i,,,]))
}
```

## Using Two Predictors

3. In Tasks 2-6 will continue our study of using two predictors, ```p167``` and ```p105```, for predicting ```digit=2``` or ```3```.

Fit 4 neural networks by minimising (sufficiently) the cross-entropy objective function. Each network has one layer of hidden units, with the number of units being 1, 2, 3, 4, respectively.

Compute their training and test errors (for the ```train``` and ```test``` sets).

```{r}
train2 = subset(train, digit==2 | digit==3, select=c("digit","p105","p167"))
trainx.mat = as.matrix(train2[,-1])
trainy = as.numeric(train2[,1])-2
trainy.mat = to_categorical(trainy, 2)
test2 = subset(test, digit==2 | digit==3, select=c("digit","p105","p167"))
testx.mat = as.matrix(test2[,-1])
testy = as.numeric(test2[,1])-2
testy.mat = to_categorical(testy, 2)
```

```{r}
errors = function(m, x1, x2, t1, t2) {
  y1 = m %>% predict(x1) %>% k_argmax() %>% as.integer()
  y2 = m %>% predict(x2) %>% k_argmax() %>% as.integer()
  c(training.error=mean(t1 != y1), test.error=mean(t2 != y2))
}
```

```{r}
unit=c(1,2,3,4)
model=vector("list",4)
history=vector("list",4)
for (i in 1:4){
  model[[i]] = 
    keras_model_sequential() %>%
    layer_dense(units=unit[i], activation="sigmoid", input_shape=c(2)) %>%
    layer_dense(units=2, activation="softmax")
  summary(model[[i]])
  model[[i]] %>% compile(loss="categorical_crossentropy",
  	                     optimizer=optimizer_rmsprop(),
  	                     metrics=c("accuracy"))
  history[[i]] <- model[[i]] %>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=32)
}
```

```{r}
sapply(1:4, function(i) errors(model[[i]], trainx.mat, testx.mat, trainy, testy))
```

4. For each of the 4 fitted neural networks obtained in Task 3, plot its decision boundary inside a scatter plot of the training data.

```{r, fig.width=10, fig.height=10, fig.align='center'}
par(mfrow=c(2,2))
yhat=vector("list",4)
s=nrow(train2)
col = as.numeric(train2$digit)
for(i in 1:4){
  plot(x=trainx.mat[,1],y=trainx.mat[,2],xlab="p105",ylab="p167",col=col,main=substitute( g* " unit",list(g = i)))
  x = seq(min(trainx.mat[,1]), max(trainx.mat[,1]),len=100)
  y = seq(min(trainx.mat[,2]), max(trainx.mat[,2]),len=100)
  z = matrix((model[[i]] %>% predict(as.matrix(expand.grid(x, y))))[,1],nrow=100)
  contour(x, y, z, levels=0.5, lwd=3, lty=1, drawlabels=F, add=T)
}
```

5. Fit a neural network with two hidden layers, with the numbers of units being (4,3), by minimising (sufficiently) the cross-entropy objective function.

Compute its training and test errors.

```{r}
model1 =
  keras_model_sequential() %>%
  layer_dense(units=4, activation="sigmoid", input_shape=c(2))%>%
  layer_dense(units=3, activation="sigmoid")%>%
  layer_dense(units=2, activation="softmax")
summary(model1)
```

```{r}
model1 %>% compile(loss="categorical_crossentropy",
	                 optimizer=optimizer_rmsprop(),
	                 metrics=c("accuracy"))
history <- model1%>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=32)
plot(history)
errors(model1, trainx.mat, testx.mat, trainy, testy)
```

6. Re-do Task 5, by setting a validation fraction of 0.3 (data needs to be shuffled first). Monitor the performance for a sufficiently long time. Find an optimal value for ```epoch``` and refit the neural network. (It is also possible to use ```callbacks```, if you know how to use them.)

Compute the training and test errors of the final neural network.

```{r}
history <- model1 %>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=32, validation_split=0.3)
plot(history)
errors(model1, trainx.mat, testx.mat, trainy, testy)
```

## Using All Predictors

7. Re-do Task 6, but with all 256 predictors used.

```{r}
train3 = subset(train, digit==2 | digit==3)
trainx.mat = as.matrix(train3[,-1])
trainy = as.numeric(train3[,1])-2
trainy.mat = to_categorical(trainy, 2)
test3 = subset(test, digit==2 | digit==3)
testx.mat = as.matrix(test3[,-1])
testy = as.numeric(test3[,1])-2
testy.mat = to_categorical(testy, 2)
```

```{r}
model2 =
  keras_model_sequential() %>%
  layer_dense(units=4, activation="sigmoid", input_shape=c(256))%>%
  layer_dense(units=3, activation="sigmoid")%>%
  layer_dense(units=2, activation="softmax")
summary(model2)
```

```{r}
model2 %>% compile(loss="categorical_crossentropy",
	                 optimizer=optimizer_rmsprop(),
	                 metrics=c("accuracy"))
history <- model2 %>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=32, validation_split=0.3)
plot(history)
errors(model2, trainx.mat, testx.mat, trainy, testy)
```

8. Consider using convolutional neural networks for predicting ```digit=2``` or ```3```, with all 256 predictors used.

Compute the training and test errors of a well-trained neural network.

```{r}
dim(trainx.mat) <- c(1389,16,16,1)
trainx.mat <- aperm(trainx.mat,c(1,3,2,4))
dim(testx.mat) <- c(364,16,16,1)
testx.mat <- aperm(testx.mat,c(1,3,2,4))
```

```{r}
model3 =
  keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(16,16,1)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=512, activation="relu") %>%
  layer_dense(units=2, activation="softmax")
summary(model3)
```

```{r}
model3 %>% compile(loss="categorical_crossentropy",
	                 optimizer=optimizer_rmsprop(),
	                 metrics=c("accuracy"))
history <- model3 %>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=128, validation_split=0.3)
errors(model3, trainx.mat, testx.mat, trainy, testy)
```

9. Re-do Task 8, but for the 10-class classification problem (```digit=0,1,...,9```).

```{r}
trainx.mat = as.matrix(train[,-1])
trainy = as.matrix(train[,1])
trainy.mat = to_categorical(trainy, 10)
dim(trainx.mat) <- c(7291,16,16,1)
trainx.mat <- aperm(trainx.mat, c(1,3,2,4))
testx.mat = as.matrix(test[,-1])
testy = as.matrix(test[,1])
testy.mat = to_categorical(testy, 10)
dim(testx.mat) <- c(2007,16,16,1)
testx.mat <- aperm(testx.mat, c(1,3,2,4))
```

```{r}
model4 =
  keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(16,16,1)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=512, activation="relu") %>%
  layer_dense(units=10, activation="softmax")
summary(model4)
```

```{r}
model4 %>% compile(loss="categorical_crossentropy",
	                 optimizer=optimizer_rmsprop(),
	                 metrics=c("accuracy"))
history <- model4 %>% fit(trainx.mat, trainy.mat, epochs=50, batch_size=128, validation_split=0.3)
errors(model4, trainx.mat, testx.mat, trainy, testy)
```

## Summary

10. Write a summary of the entire report.

In this Lab, we study the neural networks and deep learning.

We built neural networks and convolutional neural networks with the keras package.
In task 2, we generated images with numbers from the training set.

Next we investigated the performance of neural networks using 2 predictors. The results show that as the number of layers increases, the training and testing errors decrease, but overfitting is prone to occur.

We also repeated the above experiments using all predictors. The results show that 256 predictors have higher accuracy.

Finally, the performance of the convolutional neural network is also tested.